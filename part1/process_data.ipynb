{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# from nltk.stem import WordNetLemmatizer \n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words_list = stopwords.words('english')\n",
    "with open(\"stop_words\") as file:\n",
    "    stop_words_list = [line.strip() for line in file]\n",
    "\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "spacy_lemmatizer = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "file_name_list = [\"amazon.csv\",\"apple.csv\",\"google.csv\",\"facebook.csv\",\"uber.csv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Common Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 1)\n",
      "(120, 1)\n",
      "(120, 1)\n",
      "(120, 1)\n",
      "(120, 1)\n"
     ]
    }
   ],
   "source": [
    "for file_name in file_name_list:\n",
    "\n",
    "    data = pd.read_csv('data_files/cc/'+file_name,names=['content'],header=None)\n",
    "    print(data.shape)\n",
    "    with open('processed_data/cc/' + file_name, 'w') as f:\n",
    "        for index, row in data.iterrows():\n",
    "            rowdata = row[\"content\"]\n",
    "            file_clear = re.sub(\"(@[A-Za-z0-9]+)|([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", rowdata.lower())\n",
    "            file_lem = spacy_lemmatizer(file_clear)\n",
    "            file_lem = \" \".join([token.lemma_ for token in file_lem])\n",
    "            tokens = nltk.word_tokenize(file_lem)\n",
    "            filtered_words = [word for word in tokens if word not in stop_words_list]\n",
    "            for item in filtered_words:\n",
    "                f.write(\"%s \" % item)\n",
    "            f.write(\"\\n \")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35610, 88)\n",
      "(30000, 88)\n",
      "----------\n",
      "(32804, 88)\n",
      "(30000, 88)\n",
      "----------\n",
      "(35444, 88)\n",
      "(30000, 88)\n",
      "----------\n",
      "(35997, 88)\n",
      "(30000, 88)\n",
      "----------\n",
      "(32273, 88)\n",
      "(30000, 88)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "sample_size = 30000\n",
    "\n",
    "for file_name in file_name_list:\n",
    "    \n",
    "    data = pd.read_csv('data_files/twitter/'+file_name)\n",
    "    data['status_id'] = data['status_id'].str.strip()\n",
    "    data.drop_duplicates(subset='status_id', keep = 'last', inplace = True)\n",
    "    print(data.shape)\n",
    "    dt = data.sample(n = sample_size)\n",
    "    print(dt.shape)\n",
    "    print('----------')\n",
    "    \n",
    "    with open('processed_data/twitter/' + file_name, 'w') as f:\n",
    "        for index, row in dt.iterrows():\n",
    "                rowdata = row[\"text\"]\n",
    "                file_clear = re.sub(\"(@[A-Za-z0-9]+)|([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", rowdata.lower())\n",
    "                file_lem = spacy_lemmatizer(file_clear)\n",
    "                file_lem = \" \".join([token.lemma_ for token in file_lem])\n",
    "                tokens = nltk.word_tokenize(file_lem)\n",
    "                filtered_words = [word for word in tokens if word not in stop_words_list]\n",
    "                for item in filtered_words:\n",
    "                    f.write(\"%s \" % item)\n",
    "                f.write(\"\\n \")\n",
    "    f.close()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Nytimes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(950, 3)\n",
      "(300, 3)\n",
      "(457, 3)\n",
      "(300, 3)\n",
      "(1005, 3)\n",
      "(300, 3)\n",
      "(2104, 3)\n",
      "(300, 3)\n",
      "(307, 3)\n",
      "(300, 3)\n"
     ]
    }
   ],
   "source": [
    "sample_size = 300\n",
    "for file_name in file_name_list:\n",
    "    article_df = pd.read_csv('data_files/nyt/' + file_name)\n",
    "    article_df['id'] = article_df['id'].str.strip()\n",
    "    article_df.drop_duplicates(subset='id', keep = 'last', inplace = True)\n",
    "    article_df = article_df.dropna()\n",
    "    # article_df.sort_values(\"id\", inplace = True) \n",
    "    print(article_df.shape)\n",
    "    dt = article_df.sample(n = sample_size)\n",
    "    print(dt.shape)\n",
    "    print('----------')\n",
    "    \n",
    "    with open('processed_data/nyt/' + file_name, 'w') as f:\n",
    "        for index, row in dt.iterrows():\n",
    "                rowdata = row[\"content\"]\n",
    "                file_clear = re.sub(\"(@[A-Za-z0-9]+)|([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", rowdata.lower())\n",
    "                file_lem = spacy_lemmatizer(file_clear)\n",
    "                file_lem = \" \".join([token.lemma_ for token in file_lem])\n",
    "                tokens = nltk.word_tokenize(file_lem)\n",
    "                filtered_words = [word for word in tokens if word not in stop_words_list]\n",
    "                for item in filtered_words:\n",
    "                    f.write(\"%s \" % item)\n",
    "                f.write(\"\\n \")\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort and get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"amazon\"\n",
    "# data = pd.read_csv('mr_output/nyt/'+file_name, sep='\\t',header=None, names=['word','count'])\n",
    "data = pd.read_csv('mr_co_occ/cc/'+file_name, sep='\\t',header=None, names=['word','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazon-company</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>company-amazon</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>time-amazon</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amazon-time</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>time-company</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>company-time</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon-business</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>business-amazon</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>business-company</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>company-business</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>people-amazon</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazon-people</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>people-company</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>company-people</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>business-time</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>time-business</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>time-people</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>people-time</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>business-people</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>people-business</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                word  count\n",
       "3     amazon-company    106\n",
       "35    company-amazon    106\n",
       "62       time-amazon     77\n",
       "6        amazon-time     77\n",
       "66      time-company     67\n",
       "41      company-time     67\n",
       "0    amazon-business     59\n",
       "9    business-amazon     59\n",
       "12  business-company     56\n",
       "36  company-business     56\n",
       "44     people-amazon     49\n",
       "4      amazon-people     49\n",
       "48    people-company     45\n",
       "39    company-people     45\n",
       "15     business-time     40\n",
       "63     time-business     40\n",
       "67       time-people     37\n",
       "50       people-time     37\n",
       "13   business-people     34\n",
       "45   people-business     34"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values(by=['count'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## co-occurance json generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_keywords = [\"company\", \"amazon\", \"time\",\"york\",\"people\",\"trump\",\"city\",\"change\",\"president\",\"business\"]\n",
    "\n",
    "keyword_index = {}\n",
    "\n",
    "count = 0\n",
    "for k in top_keywords:\n",
    "    keyword_index[k] = count\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company': 0,\n",
       " 'amazon': 1,\n",
       " 'time': 2,\n",
       " 'york': 3,\n",
       " 'people': 4,\n",
       " 'trump': 5,\n",
       " 'city': 6,\n",
       " 'change': 7,\n",
       " 'president': 8,\n",
       " 'business': 9}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"amazon\"\n",
    "\n",
    "data = pd.read_csv('mr_co_occ/twitter/'+file_name, sep='\\t',header=None, names=['count','word'])\n",
    "\n",
    "json_dic ={}\n",
    "\n",
    "\n",
    "lst = []\n",
    "for index, row in data.iterrows():\n",
    "    node = {}\n",
    "    in_1 = keyword_index[row['word'].split('-')[0]]\n",
    "    in_2 = keyword_index[row['word'].split('-')[1]]\n",
    "    \n",
    "    node['source'] = in_1\n",
    "    node['target'] = in_2\n",
    "    node['value'] = row['count']\n",
    "    lst.append(node)\n",
    "    \n",
    "json_dic['links'] = lst  \n",
    "\n",
    "lst = []\n",
    "for ky,vl in keyword_index.items():\n",
    "    node = {}\n",
    "    node['group'] = 'humanas'\n",
    "    node['index'] = vl\n",
    "    node['name'] = ky\n",
    "    lst.append(node)\n",
    "    \n",
    "json_dic['nodes'] = lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"links\": [{\"source\": 2, \"target\": 1, \"value\": 363}, {\"source\": 1, \"target\": 2, \"value\": 363}, {\"source\": 0, \"target\": 1, \"value\": 218}, {\"source\": 1, \"target\": 0, \"value\": 218}, {\"source\": 4, \"target\": 1, \"value\": 157}, {\"source\": 1, \"target\": 4, \"value\": 157}, {\"source\": 9, \"target\": 1, \"value\": 138}, {\"source\": 1, \"target\": 9, \"value\": 138}, {\"source\": 5, \"target\": 1, \"value\": 107}, {\"source\": 1, \"target\": 5, \"value\": 107}, {\"source\": 7, \"target\": 1, \"value\": 70}, {\"source\": 1, \"target\": 7, \"value\": 70}, {\"source\": 6, \"target\": 1, \"value\": 63}, {\"source\": 1, \"target\": 6, \"value\": 63}, {\"source\": 2, \"target\": 4, \"value\": 45}, {\"source\": 4, \"target\": 2, \"value\": 45}, {\"source\": 3, \"target\": 1, \"value\": 41}, {\"source\": 1, \"target\": 3, \"value\": 41}, {\"source\": 8, \"target\": 1, \"value\": 29}, {\"source\": 1, \"target\": 8, \"value\": 29}, {\"source\": 5, \"target\": 0, \"value\": 28}, {\"source\": 0, \"target\": 5, \"value\": 28}, {\"source\": 4, \"target\": 0, \"value\": 25}, {\"source\": 0, \"target\": 4, \"value\": 25}, {\"source\": 2, \"target\": 7, \"value\": 24}, {\"source\": 7, \"target\": 2, \"value\": 24}, {\"source\": 2, \"target\": 0, \"value\": 17}, {\"source\": 0, \"target\": 2, \"value\": 17}, {\"source\": 3, \"target\": 6, \"value\": 16}, {\"source\": 6, \"target\": 3, \"value\": 16}, {\"source\": 5, \"target\": 8, \"value\": 15}, {\"source\": 8, \"target\": 5, \"value\": 15}, {\"source\": 4, \"target\": 7, \"value\": 14}, {\"source\": 7, \"target\": 4, \"value\": 14}, {\"source\": 3, \"target\": 2, \"value\": 13}, {\"source\": 2, \"target\": 3, \"value\": 13}, {\"source\": 0, \"target\": 7, \"value\": 13}, {\"source\": 0, \"target\": 9, \"value\": 13}, {\"source\": 7, \"target\": 0, \"value\": 13}, {\"source\": 9, \"target\": 0, \"value\": 13}, {\"source\": 2, \"target\": 9, \"value\": 12}, {\"source\": 9, \"target\": 2, \"value\": 12}, {\"source\": 5, \"target\": 4, \"value\": 11}, {\"source\": 4, \"target\": 5, \"value\": 11}, {\"source\": 4, \"target\": 9, \"value\": 11}, {\"source\": 9, \"target\": 4, \"value\": 11}, {\"source\": 5, \"target\": 2, \"value\": 9}, {\"source\": 2, \"target\": 5, \"value\": 9}, {\"source\": 2, \"target\": 6, \"value\": 9}, {\"source\": 6, \"target\": 2, \"value\": 9}, {\"source\": 4, \"target\": 6, \"value\": 6}, {\"source\": 6, \"target\": 4, \"value\": 6}, {\"source\": 2, \"target\": 8, \"value\": 5}, {\"source\": 8, \"target\": 2, \"value\": 5}, {\"source\": 7, \"target\": 9, \"value\": 5}, {\"source\": 9, \"target\": 7, \"value\": 5}, {\"source\": 8, \"target\": 0, \"value\": 4}, {\"source\": 0, \"target\": 8, \"value\": 4}, {\"source\": 3, \"target\": 4, \"value\": 3}, {\"source\": 5, \"target\": 9, \"value\": 3}, {\"source\": 8, \"target\": 4, \"value\": 3}, {\"source\": 4, \"target\": 3, \"value\": 3}, {\"source\": 4, \"target\": 8, \"value\": 3}, {\"source\": 9, \"target\": 5, \"value\": 3}, {\"source\": 3, \"target\": 8, \"value\": 2}, {\"source\": 5, \"target\": 7, \"value\": 2}, {\"source\": 8, \"target\": 3, \"value\": 2}, {\"source\": 6, \"target\": 9, \"value\": 2}, {\"source\": 7, \"target\": 5, \"value\": 2}, {\"source\": 9, \"target\": 6, \"value\": 2}, {\"source\": 3, \"target\": 0, \"value\": 1}, {\"source\": 3, \"target\": 9, \"value\": 1}, {\"source\": 5, \"target\": 6, \"value\": 1}, {\"source\": 8, \"target\": 6, \"value\": 1}, {\"source\": 8, \"target\": 7, \"value\": 1}, {\"source\": 0, \"target\": 3, \"value\": 1}, {\"source\": 0, \"target\": 6, \"value\": 1}, {\"source\": 6, \"target\": 5, \"value\": 1}, {\"source\": 6, \"target\": 8, \"value\": 1}, {\"source\": 6, \"target\": 0, \"value\": 1}, {\"source\": 6, \"target\": 7, \"value\": 1}, {\"source\": 7, \"target\": 8, \"value\": 1}, {\"source\": 7, \"target\": 6, \"value\": 1}, {\"source\": 9, \"target\": 3, \"value\": 1}], \"nodes\": [{\"group\": \"humanas\", \"index\": 0, \"name\": \"company\"}, {\"group\": \"humanas\", \"index\": 1, \"name\": \"amazon\"}, {\"group\": \"humanas\", \"index\": 2, \"name\": \"time\"}, {\"group\": \"humanas\", \"index\": 3, \"name\": \"york\"}, {\"group\": \"humanas\", \"index\": 4, \"name\": \"people\"}, {\"group\": \"humanas\", \"index\": 5, \"name\": \"trump\"}, {\"group\": \"humanas\", \"index\": 6, \"name\": \"city\"}, {\"group\": \"humanas\", \"index\": 7, \"name\": \"change\"}, {\"group\": \"humanas\", \"index\": 8, \"name\": \"president\"}, {\"group\": \"humanas\", \"index\": 9, \"name\": \"business\"}]}'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json_data = json.dumps(json_dic)\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
