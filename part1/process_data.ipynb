{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# from nltk.stem import WordNetLemmatizer \n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words_list = stopwords.words('english')\n",
    "with open(\"stop_words\") as file:\n",
    "    stop_words_list = [line.strip() for line in file]\n",
    "\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "spacy_lemmatizer = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35610, 88)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"amazon.csv\"\n",
    "sample_size = 30000\n",
    "data = pd.read_csv('data_files/twitter/'+file_name)\n",
    "\n",
    "data['status_id'] = data['status_id'].str.strip()\n",
    "data.drop_duplicates(subset='status_id', keep = 'last', inplace = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 88)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = data.sample(n = sample_size)\n",
    "dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_data/twitter/' + file_name, 'w') as f:\n",
    "    for index, row in dt.iterrows():\n",
    "            rowdata = row[\"text\"]\n",
    "            file_clear = re.sub(\"(@[A-Za-z0-9]+)|([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", rowdata.lower())\n",
    "            file_lem = spacy_lemmatizer(file_clear)\n",
    "            file_lem = \" \".join([token.lemma_ for token in file_lem])\n",
    "            tokens = nltk.word_tokenize(file_lem)\n",
    "            filtered_words = [word for word in tokens if word not in stop_words_list]\n",
    "            for item in filtered_words:\n",
    "                f.write(\"%s \" % item)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Nytimes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'nyt_apple.csv'\n",
    "sample_size = 300\n",
    "article_df = pd.read_csv('data_files/nyt/' + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(457, 3)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df['id'] = article_df['id'].str.strip()\n",
    "article_df.drop_duplicates(subset='id', keep = 'last', inplace = True)\n",
    "article_df = article_df.dropna()\n",
    "# article_df.sort_values(\"id\", inplace = True) \n",
    "article_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 3)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = article_df.sample(n = sample_size)\n",
    "dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_data/nyt/' + file_name, 'w') as f:\n",
    "    for index, row in dt.iterrows():\n",
    "            rowdata = row[\"content\"]\n",
    "            file_clear = re.sub(\"(@[A-Za-z0-9]+)|([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", rowdata.lower())\n",
    "            file_lem = spacy_lemmatizer(file_clear)\n",
    "            file_lem = \" \".join([token.lemma_ for token in file_lem])\n",
    "            tokens = nltk.word_tokenize(file_lem)\n",
    "            filtered_words = [word for word in tokens if word not in stop_words_list]\n",
    "            for item in filtered_words:\n",
    "                f.write(\"%s \" % item)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort and get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"uber\"\n",
    "# data = pd.read_csv('mr_output/nyt/'+file_name, sep='\\t',header=None, names=['word','count'])\n",
    "data = pd.read_csv('mr_output/twitter/'+file_name, sep='\\t',header=None, names=['word','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19066</th>\n",
       "      <td>uber</td>\n",
       "      <td>27449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5500</th>\n",
       "      <td>driver</td>\n",
       "      <td>6721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10991</th>\n",
       "      <td>lyft</td>\n",
       "      <td>4129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9450</th>\n",
       "      <td>ipo</td>\n",
       "      <td>2780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15383</th>\n",
       "      <td>ride</td>\n",
       "      <td>2461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5702</th>\n",
       "      <td>eat</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5496</th>\n",
       "      <td>drive</td>\n",
       "      <td>1870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2777</th>\n",
       "      <td>car</td>\n",
       "      <td>1808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18487</th>\n",
       "      <td>time</td>\n",
       "      <td>1619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>billion</td>\n",
       "      <td>1594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17748</th>\n",
       "      <td>support</td>\n",
       "      <td>1572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5357</th>\n",
       "      <td>don</td>\n",
       "      <td>1572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19821</th>\n",
       "      <td>via</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>amp</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3701</th>\n",
       "      <td>company</td>\n",
       "      <td>1125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13515</th>\n",
       "      <td>pay</td>\n",
       "      <td>1073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13625</th>\n",
       "      <td>people</td>\n",
       "      <td>1069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3544</th>\n",
       "      <td>code</td>\n",
       "      <td>1031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14807</th>\n",
       "      <td>re</td>\n",
       "      <td>1023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14494</th>\n",
       "      <td>public</td>\n",
       "      <td>1014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  count\n",
       "19066     uber  27449\n",
       "5500    driver   6721\n",
       "10991     lyft   4129\n",
       "9450       ipo   2780\n",
       "15383     ride   2461\n",
       "5702       eat   1975\n",
       "5496     drive   1870\n",
       "2777       car   1808\n",
       "18487     time   1619\n",
       "1812   billion   1594\n",
       "17748  support   1572\n",
       "5357       don   1572\n",
       "19821      via   1506\n",
       "612        amp   1199\n",
       "3701   company   1125\n",
       "13515      pay   1073\n",
       "13625   people   1069\n",
       "3544      code   1031\n",
       "14807       re   1023\n",
       "14494   public   1014"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values(by=['count'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
