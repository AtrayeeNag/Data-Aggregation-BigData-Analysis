{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# from nltk.stem import WordNetLemmatizer \n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words_list = stopwords.words('english')\n",
    "with open(\"stop_words\") as file:\n",
    "    stop_words_list = [line.strip() for line in file]\n",
    "\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "spacy_lemmatizer = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "file_name_list = [\"amazon.csv\",\"apple.csv\",\"google.csv\",\"facebook.csv\",\"uber.csv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Common Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 1)\n",
      "(120, 1)\n",
      "(120, 1)\n",
      "(120, 1)\n",
      "(120, 1)\n"
     ]
    }
   ],
   "source": [
    "for file_name in file_name_list:\n",
    "\n",
    "    data = pd.read_csv('data_files/cc/'+file_name,names=['content'],header=None)\n",
    "    print(data.shape)\n",
    "    with open('processed_data/cc/' + file_name, 'w') as f:\n",
    "        for index, row in data.iterrows():\n",
    "            rowdata = row[\"content\"]\n",
    "            file_clear = re.sub(\"(@[A-Za-z0-9]+)|([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", rowdata.lower())\n",
    "            file_lem = spacy_lemmatizer(file_clear)\n",
    "            file_lem = \" \".join([token.lemma_ for token in file_lem])\n",
    "            tokens = nltk.word_tokenize(file_lem)\n",
    "            filtered_words = [word for word in tokens if word not in stop_words_list]\n",
    "            for item in filtered_words:\n",
    "                f.write(\"%s \" % item)\n",
    "            f.write(\"\\n \")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35610, 88)\n",
      "(30000, 88)\n",
      "----------\n",
      "(32804, 88)\n",
      "(30000, 88)\n",
      "----------\n",
      "(35444, 88)\n",
      "(30000, 88)\n",
      "----------\n",
      "(35997, 88)\n",
      "(30000, 88)\n",
      "----------\n",
      "(32273, 88)\n",
      "(30000, 88)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "sample_size = 30000\n",
    "\n",
    "for file_name in file_name_list:\n",
    "    \n",
    "    data = pd.read_csv('data_files/twitter/'+file_name)\n",
    "    data['status_id'] = data['status_id'].str.strip()\n",
    "    data.drop_duplicates(subset='status_id', keep = 'last', inplace = True)\n",
    "    print(data.shape)\n",
    "    dt = data.sample(n = sample_size)\n",
    "    print(dt.shape)\n",
    "    print('----------')\n",
    "    \n",
    "    with open('processed_data/twitter/' + file_name, 'w') as f:\n",
    "        for index, row in dt.iterrows():\n",
    "                rowdata = row[\"text\"]\n",
    "                file_clear = re.sub(\"(@[A-Za-z0-9]+)|([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", rowdata.lower())\n",
    "                file_lem = spacy_lemmatizer(file_clear)\n",
    "                file_lem = \" \".join([token.lemma_ for token in file_lem])\n",
    "                tokens = nltk.word_tokenize(file_lem)\n",
    "                filtered_words = [word for word in tokens if word not in stop_words_list]\n",
    "                for item in filtered_words:\n",
    "                    f.write(\"%s \" % item)\n",
    "                f.write(\"\\n \")\n",
    "    f.close()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Nytimes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(950, 3)\n",
      "(300, 3)\n",
      "(457, 3)\n",
      "(300, 3)\n",
      "(1005, 3)\n",
      "(300, 3)\n",
      "(2104, 3)\n",
      "(300, 3)\n",
      "(307, 3)\n",
      "(300, 3)\n"
     ]
    }
   ],
   "source": [
    "sample_size = 300\n",
    "for file_name in file_name_list:\n",
    "    article_df = pd.read_csv('data_files/nyt/' + file_name)\n",
    "    article_df['id'] = article_df['id'].str.strip()\n",
    "    article_df.drop_duplicates(subset='id', keep = 'last', inplace = True)\n",
    "    article_df = article_df.dropna()\n",
    "    # article_df.sort_values(\"id\", inplace = True) \n",
    "    print(article_df.shape)\n",
    "    dt = article_df.sample(n = sample_size)\n",
    "    print(dt.shape)\n",
    "    print('----------')\n",
    "    \n",
    "    with open('processed_data/nyt/' + file_name, 'w') as f:\n",
    "        for index, row in dt.iterrows():\n",
    "                rowdata = row[\"content\"]\n",
    "                file_clear = re.sub(\"(@[A-Za-z0-9]+)|([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", rowdata.lower())\n",
    "                file_lem = spacy_lemmatizer(file_clear)\n",
    "                file_lem = \" \".join([token.lemma_ for token in file_lem])\n",
    "                tokens = nltk.word_tokenize(file_lem)\n",
    "                filtered_words = [word for word in tokens if word not in stop_words_list]\n",
    "                for item in filtered_words:\n",
    "                    f.write(\"%s \" % item)\n",
    "                f.write(\"\\n \")\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort and get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"uber\"\n",
    "# data = pd.read_csv('mr_output/nyt/'+file_name, sep='\\t',header=None, names=['word','count'])\n",
    "data = pd.read_csv('mr_output/cc/'+file_name, sep='\\t',header=None, names=['word','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>company</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4703</th>\n",
       "      <td>people</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4074</th>\n",
       "      <td>million</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6671</th>\n",
       "      <td>uber</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>billion</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5728</th>\n",
       "      <td>service</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7203</th>\n",
       "      <td>zuckerberg</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3919</th>\n",
       "      <td>mark</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3479</th>\n",
       "      <td>jonathan</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7197</th>\n",
       "      <td>zittrain</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>business</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6091</th>\n",
       "      <td>startup</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6500</th>\n",
       "      <td>time</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2630</th>\n",
       "      <td>fund</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>raise</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>app</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3920</th>\n",
       "      <td>market</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>capital</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>build</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>car</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  count\n",
       "1216     company    630\n",
       "4703      people    353\n",
       "4074     million    343\n",
       "6671        uber    269\n",
       "667      billion    255\n",
       "5728     service    254\n",
       "7203  zuckerberg    253\n",
       "3919        mark    250\n",
       "3479    jonathan    234\n",
       "7197    zittrain    233\n",
       "853     business    232\n",
       "6091     startup    228\n",
       "6500        time    227\n",
       "2630        fund    188\n",
       "5168       raise    182\n",
       "333          app    180\n",
       "3920      market    164\n",
       "908      capital    161\n",
       "826        build    154\n",
       "917          car    146"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values(by=['count'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
