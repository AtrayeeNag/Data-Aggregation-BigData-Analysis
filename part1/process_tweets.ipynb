{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_files/1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_txt = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twt = pd.DataFrame(tweets_txt)\n",
    "df_twt.to_csv('processed_data/hindu.txt', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"how much for the maple syrup          That's @ridiculous   \""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"how much for the maple syrup? $20.99? That's @ridiculous!!!\"\n",
    "s_an = re.sub(r'[^a-zA-Z\\'@]', ' ', s)\n",
    "\n",
    "s_an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how',\n",
       " 'much',\n",
       " 'for',\n",
       " 'the',\n",
       " 'maple',\n",
       " 'syrup',\n",
       " '20',\n",
       " '99',\n",
       " 'That',\n",
       " 's',\n",
       " 'ridiculous']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grow'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how\n",
      "much\n",
      "for\n",
      "the\n",
      "maple\n",
      "syrup\n",
      "20\n",
      "99\n",
      "That\n",
      "s\n",
      "ridiculous\n"
     ]
    }
   ],
   "source": [
    "pst = PorterStemmer()\n",
    "pst.stem('grows')\n",
    "for tk in tokens:\n",
    "    print(tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['much', 'maple', 'syrup', '20', '99', 'That', 'ridiculous']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words = [word for word in tokens if word not in stopwords.words('english')]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'I',\n",
       " 'remembered',\n",
       " 'another',\n",
       " 'I',\n",
       " 'heart',\n",
       " 'recently',\n",
       " 'broken',\n",
       " 'got',\n",
       " 'little',\n",
       " 'Ganesha',\n",
       " 'figurine',\n",
       " 'blessed',\n",
       " 'Hindu',\n",
       " 'temple',\n",
       " 'niece',\n",
       " 'anaprasana',\n",
       " 'My',\n",
       " 'Catholic',\n",
       " 'mother',\n",
       " 'wanted',\n",
       " 'know',\n",
       " 'I',\n",
       " 'get',\n",
       " 'little',\n",
       " 'St',\n",
       " 'Anthony',\n",
       " 'instead',\n",
       " 'What',\n",
       " 'I',\n",
       " 'asked',\n",
       " 'Lost',\n",
       " 'causes',\n",
       " 'said',\n",
       " 'Noah',\n",
       " 'flood',\n",
       " 'story',\n",
       " 'also',\n",
       " 'mirrors',\n",
       " 'Manu',\n",
       " 'flood',\n",
       " 'story',\n",
       " 'Hinduism',\n",
       " 'As',\n",
       " 'well',\n",
       " 'Deucalion',\n",
       " 'flood',\n",
       " 'story',\n",
       " 'Greek',\n",
       " 'Mythology',\n",
       " 'There',\n",
       " 'millions',\n",
       " 'Hinduism',\n",
       " 'alone',\n",
       " 'really',\n",
       " 'people',\n",
       " 'punjabi',\n",
       " 'others',\n",
       " 'tell',\n",
       " 'I',\n",
       " 'say',\n",
       " 'I',\n",
       " 'punjabi',\n",
       " 'I',\n",
       " 'Hindu',\n",
       " 'mom',\n",
       " 'part',\n",
       " 'punjab',\n",
       " 'went',\n",
       " 'himachal',\n",
       " '1966',\n",
       " 'mom',\n",
       " '9',\n",
       " 'Check',\n",
       " 'Don',\n",
       " 'oppose',\n",
       " 'Modi',\n",
       " 'Hindu',\n",
       " 'Dei',\n",
       " 'pvndai',\n",
       " 'pannadai',\n",
       " 'I',\n",
       " 'Hindu',\n",
       " 'proud',\n",
       " 'one',\n",
       " 'call',\n",
       " 'Sanghi',\n",
       " 'ever',\n",
       " 'You',\n",
       " 'name',\n",
       " 'God',\n",
       " 'sound',\n",
       " 'like',\n",
       " 'ready',\n",
       " 'give',\n",
       " 'Bjs',\n",
       " 'Thimuka',\n",
       " 'kudumbam',\n",
       " 'More',\n",
       " 'Hindu',\n",
       " 'girls',\n",
       " 'kidnapped',\n",
       " 'Sugathan',\n",
       " 'I',\n",
       " 'Hindu',\n",
       " 'call',\n",
       " 'whatever',\n",
       " 'You',\n",
       " 'nothing',\n",
       " 'wipe',\n",
       " 'Another',\n",
       " 'set',\n",
       " 'Hindu',\n",
       " 'girls',\n",
       " 'kidnapped',\n",
       " 'forcibly',\n",
       " 'converted',\n",
       " 'married',\n",
       " 'I',\n",
       " 'guess',\n",
       " 'agency',\n",
       " 'talk',\n",
       " 'Islamist',\n",
       " 'terrorist',\n",
       " 'kidnap',\n",
       " 'Hindu',\n",
       " 'women',\n",
       " 'forcibly',\n",
       " 'convert',\n",
       " 'hindu',\n",
       " 'My',\n",
       " 'sources',\n",
       " 'told',\n",
       " 'yeh',\n",
       " 'news',\n",
       " 'sunte',\n",
       " 'hi',\n",
       " 'idhar',\n",
       " 'India',\n",
       " 'Congis',\n",
       " 'Commies',\n",
       " 'k',\n",
       " 'firse',\n",
       " 'phatke',\n",
       " 'flower',\n",
       " 'ho',\n",
       " 'gaya',\n",
       " 'hindu',\n",
       " 'But',\n",
       " 'No',\n",
       " 'one',\n",
       " 'wants',\n",
       " 'know',\n",
       " 'chor',\n",
       " 'Nehru',\n",
       " 'family',\n",
       " 'doormat',\n",
       " 'distorian',\n",
       " 'like',\n",
       " 'Irfan',\n",
       " 'habib',\n",
       " 'hindu',\n",
       " 'Good',\n",
       " 'No',\n",
       " 'compromise',\n",
       " 'national',\n",
       " 'security',\n",
       " 'name',\n",
       " 'journalism',\n",
       " 'transparency',\n",
       " 'hindu',\n",
       " 'Jet',\n",
       " 'Airways',\n",
       " 'multi',\n",
       " 'crore',\n",
       " 'scam',\n",
       " 'scamsters',\n",
       " 'Praful',\n",
       " 'Patel',\n",
       " 'Sharad',\n",
       " 'Pawar',\n",
       " 'Naresh',\n",
       " 'Goel',\n",
       " 'course',\n",
       " '10',\n",
       " 'Janpath',\n",
       " 'Dawood',\n",
       " 'foreign',\n",
       " 'partner']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1 = open(\"exp/hindu.txt\",\"r+\")  \n",
    "  \n",
    "# print \"Output of Read function is \"\n",
    "file_data = file1.read()\n",
    "# remove url\n",
    "file_clear = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", file_data[0:2000])\n",
    "\n",
    "# # remove twitter usernames\n",
    "# data = re.sub(r\"\\@\\S+\", \"\", data)\n",
    "\n",
    "# file_clear = re.sub(r'[^a-zA-Z\\'@]', ' ', fl_wo_url)\n",
    "\n",
    "\n",
    "tokens = nltk.word_tokenize(file_clear)\n",
    "filtered_words = [word for word in tokens if word not in stopwords.words('english')]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "I\n",
      "rememb\n",
      "anoth\n",
      "I\n",
      "heart\n",
      "recent\n",
      "broken\n",
      "got\n",
      "littl\n",
      "ganesha\n",
      "figurin\n",
      "bless\n",
      "hindu\n",
      "templ\n",
      "niec\n",
      "anaprasana\n",
      "My\n",
      "cathol\n",
      "mother\n",
      "want\n",
      "know\n",
      "I\n",
      "get\n",
      "littl\n",
      "St\n",
      "anthoni\n",
      "instead\n",
      "what\n",
      "I\n",
      "ask\n",
      "lost\n",
      "caus\n",
      "said\n",
      "noah\n",
      "flood\n",
      "stori\n",
      "also\n",
      "mirror\n",
      "manu\n",
      "flood\n",
      "stori\n",
      "hinduism\n",
      "As\n",
      "well\n",
      "deucalion\n",
      "flood\n",
      "stori\n",
      "greek\n",
      "mytholog\n",
      "there\n",
      "million\n",
      "hinduism\n",
      "alon\n",
      "realli\n",
      "peopl\n",
      "punjabi\n",
      "other\n",
      "tell\n",
      "I\n",
      "say\n",
      "I\n",
      "punjabi\n",
      "I\n",
      "hindu\n",
      "mom\n",
      "part\n",
      "punjab\n",
      "went\n",
      "himach\n",
      "1966\n",
      "mom\n",
      "9\n",
      "check\n",
      "don\n",
      "oppos\n",
      "modi\n",
      "hindu\n",
      "dei\n",
      "pvndai\n",
      "pannadai\n",
      "I\n",
      "hindu\n",
      "proud\n",
      "one\n",
      "call\n",
      "sanghi\n",
      "ever\n",
      "you\n",
      "name\n",
      "god\n",
      "sound\n",
      "like\n",
      "readi\n",
      "give\n",
      "bj\n",
      "thimuka\n",
      "kudumbam\n",
      "more\n",
      "hindu\n",
      "girl\n",
      "kidnap\n",
      "sugathan\n",
      "I\n",
      "hindu\n",
      "call\n",
      "whatev\n",
      "you\n",
      "noth\n",
      "wipe\n",
      "anoth\n",
      "set\n",
      "hindu\n",
      "girl\n",
      "kidnap\n",
      "forcibl\n",
      "convert\n",
      "marri\n",
      "I\n",
      "guess\n",
      "agenc\n",
      "talk\n",
      "islamist\n",
      "terrorist\n",
      "kidnap\n",
      "hindu\n",
      "women\n",
      "forcibl\n",
      "convert\n",
      "hindu\n",
      "My\n",
      "sourc\n",
      "told\n",
      "yeh\n",
      "news\n",
      "sunt\n",
      "hi\n",
      "idhar\n",
      "india\n",
      "congi\n",
      "commi\n",
      "k\n",
      "firs\n",
      "phatk\n",
      "flower\n",
      "ho\n",
      "gaya\n",
      "hindu\n",
      "but\n",
      "No\n",
      "one\n",
      "want\n",
      "know\n",
      "chor\n",
      "nehru\n",
      "famili\n",
      "doormat\n",
      "distorian\n",
      "like\n",
      "irfan\n",
      "habib\n",
      "hindu\n",
      "good\n",
      "No\n",
      "compromis\n",
      "nation\n",
      "secur\n",
      "name\n",
      "journal\n",
      "transpar\n",
      "hindu\n",
      "jet\n",
      "airway\n",
      "multi\n",
      "crore\n",
      "scam\n",
      "scamster\n",
      "praful\n",
      "patel\n",
      "sharad\n",
      "pawar\n",
      "naresh\n",
      "goel\n",
      "cours\n",
      "10\n",
      "janpath\n",
      "dawood\n",
      "foreign\n",
      "partner\n"
     ]
    }
   ],
   "source": [
    "pst = PorterStemmer()\n",
    "\n",
    "for tk in filtered_words:\n",
    "    print(pst.stem(tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text    I remembered another  I d had my heart recently broken so got a little Ganesha figurine  which was blessed at the Hindu temple at my niece s anaprasana  My Catholic mother wanted to know why I didn t get a little St  Anthony instead  What s he for  I asked  Lost causes  she said    Noah s flood story also mirrors the Manu flood story in Hinduism    As well as the Deucalion flood story in Greek Mythology            There are millions in Hinduism alone    there are really people out here  some punjabi and others not  who tell me I can t say I m punjabi because I m Hindu and because my mom s from the part of punjab that went into himachal in 1966  when my mom was 9          Check this out  Don t oppose Modi just because he is a Hindu          Dei pvndai pannadai I am Hindu and a proud one  call me Sanghi or what ever  You have a name of God and you sound like you are ready to give Bjs to Thimuka kudumbam       More Hindu girls are being kidnapped      Sugathan     I am a Hindu and call me whatever  You are nothing but an     wipe      Another set of Hindu girls kidnapped  forcibly converted and married off       I guess your agency is all talk  Islamist terrorist kidnap Hindu women and forcibly convert them       hindu My sources told me that   yeh news sunte hi idhar India me Congis and Commies k firse   phatke flower ho gaya        hindu But   No one wants to know that from any chor Nehru family s doormat distorian like Irfan habib        hindu Good   No compromise with national security in the name of journalism or transparency        hindu Jet Airways is a multi crore scam    and scamsters are    Praful Patel  Sharad Pawar  Naresh Goel    and of course 10 Janpath    Dawood was foreign partner       '"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text\\n\"@AlannaBennett I remembered another. I\\'d had my heart recently broken so got a little Ganesha figurine, which was blessed at the Hindu temple at my niece\\'s anaprasana. My Catholic mother wanted to know why I didn\\'t get a little St. Anthony instead. What\\'s he for? I asked. Lost causes, she said.\"\\n\"Noah\\'s flood story also mirrors the Manu flood story in Hinduism. \\n\\nAs well as the Deucalion flood story in Greek Mythology.\"\\n\"@DenizSayak @LordLogiq @godFreeWorld @AApologetics There are millions in Hinduism alone.\"\\n\"there are really people out here (some punjabi and others not) who tell me I can’t say I’m punjabi because I’m Hindu and because my mom’s from the part of punjab that went into himachal in 1966 (when my mom was 9)\"\\n\"@rosammat @GabbbarSingh @ChandrusWeb Check this out. Don\\'t oppose Modi just because he is a Hindu.\\n\\nhttps://t.co/5KaBqld8BD\"\\n\"@manitsk @mahesh10816 Dei pvndai pannadai I am Hindu and a proud one, call me Sanghi or what ever! You have a name of God and you sound like you are ready to give Bjs to Thimuka kudumbam\"\\n\"@ShahHossain @taslimanasreen More Hindu girls are being kidnapped.\"\\n\"@Tharun_Sugathan @shilpamdas @smritiirani I am a Hindu and call me whatever. You are nothing but an @$$ wipe.\"\\n\"@jihadwatchRS Another set of Hindu girls kidnapped, forcibly converted and married off. https://t.co/Up63a0Zbkf\"\\n\"@UNHumanRights I guess your agency is all talk. Islamist terrorist kidnap Hindu women and forcibly convert them. https://t.co/b7MaWohI91\"\\n\"@the_hindu My sources told me that...yeh news sunte hi idhar India me Congis and Commies k firse...phatke flower ho gaya...\"\\n\"@the_hindu But...No one wants to know that from any chor Nehru family\\'s doormat distorian like Irfan habib...\"\\n\"@the_hindu Good...No compromise with national security in the name of journalism or transparency...\"\\n\"@the_hindu Jet Airways is a multi crore scam... and scamsters are... Praful Patel, Sharad Pawar, Naresh Goel... and of course 10 Janpath... Dawood was foreign partner...\"\\n\"@'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_data[0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
