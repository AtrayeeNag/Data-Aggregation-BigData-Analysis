{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# from nltk.stem import WordNetLemmatizer \n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words_list = stopwords.words('english')\n",
    "with open(\"stop_words\") as file:\n",
    "    stop_words_list = [line.strip() for line in file]\n",
    "\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "spacy_lemmatizer = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "file_name_list = [\"amazon.csv\",\"apple.csv\",\"google.csv\",\"facebook.csv\",\"uber.csv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Common Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 1)\n",
      "(120, 1)\n",
      "(120, 1)\n",
      "(120, 1)\n",
      "(120, 1)\n"
     ]
    }
   ],
   "source": [
    "for file_name in file_name_list:\n",
    "\n",
    "    data = pd.read_csv('data_files/cc/'+file_name,names=['content'],header=None)\n",
    "    print(data.shape)\n",
    "    with open('processed_data/cc/' + file_name, 'w') as f:\n",
    "        for index, row in data.iterrows():\n",
    "            rowdata = row[\"content\"]\n",
    "            file_clear = re.sub(\"(@[A-Za-z0-9]+)|([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", rowdata.lower())\n",
    "            file_lem = spacy_lemmatizer(file_clear)\n",
    "            file_lem = \" \".join([token.lemma_ for token in file_lem])\n",
    "            tokens = nltk.word_tokenize(file_lem)\n",
    "            filtered_words = [word for word in tokens if word not in stop_words_list]\n",
    "            for item in filtered_words:\n",
    "                f.write(\"%s \" % item)\n",
    "            f.write(\"\\n \")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35610, 88)\n",
      "(30000, 88)\n",
      "----------\n",
      "(32804, 88)\n",
      "(30000, 88)\n",
      "----------\n",
      "(35444, 88)\n",
      "(30000, 88)\n",
      "----------\n",
      "(35997, 88)\n",
      "(30000, 88)\n",
      "----------\n",
      "(32273, 88)\n",
      "(30000, 88)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "sample_size = 30000\n",
    "\n",
    "for file_name in file_name_list:\n",
    "    \n",
    "    data = pd.read_csv('data_files/twitter/'+file_name)\n",
    "    data['status_id'] = data['status_id'].str.strip()\n",
    "    data.drop_duplicates(subset='status_id', keep = 'last', inplace = True)\n",
    "    print(data.shape)\n",
    "    dt = data.sample(n = sample_size)\n",
    "    print(dt.shape)\n",
    "    print('----------')\n",
    "    \n",
    "    with open('processed_data/twitter/' + file_name, 'w') as f:\n",
    "        for index, row in dt.iterrows():\n",
    "                rowdata = row[\"text\"]\n",
    "                file_clear = re.sub(\"(@[A-Za-z0-9]+)|([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", rowdata.lower())\n",
    "                file_lem = spacy_lemmatizer(file_clear)\n",
    "                file_lem = \" \".join([token.lemma_ for token in file_lem])\n",
    "                tokens = nltk.word_tokenize(file_lem)\n",
    "                filtered_words = [word for word in tokens if word not in stop_words_list]\n",
    "                for item in filtered_words:\n",
    "                    f.write(\"%s \" % item)\n",
    "                f.write(\"\\n \")\n",
    "    f.close()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Nytimes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(950, 3)\n",
      "(300, 3)\n",
      "(457, 3)\n",
      "(300, 3)\n",
      "(1005, 3)\n",
      "(300, 3)\n",
      "(2104, 3)\n",
      "(300, 3)\n",
      "(307, 3)\n",
      "(300, 3)\n"
     ]
    }
   ],
   "source": [
    "sample_size = 300\n",
    "for file_name in file_name_list:\n",
    "    article_df = pd.read_csv('data_files/nyt/' + file_name)\n",
    "    article_df['id'] = article_df['id'].str.strip()\n",
    "    article_df.drop_duplicates(subset='id', keep = 'last', inplace = True)\n",
    "    article_df = article_df.dropna()\n",
    "    # article_df.sort_values(\"id\", inplace = True) \n",
    "    print(article_df.shape)\n",
    "    dt = article_df.sample(n = sample_size)\n",
    "    print(dt.shape)\n",
    "    print('----------')\n",
    "    \n",
    "    with open('processed_data/nyt/' + file_name, 'w') as f:\n",
    "        for index, row in dt.iterrows():\n",
    "                rowdata = row[\"content\"]\n",
    "                file_clear = re.sub(\"(@[A-Za-z0-9]+)|([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", rowdata.lower())\n",
    "                file_lem = spacy_lemmatizer(file_clear)\n",
    "                file_lem = \" \".join([token.lemma_ for token in file_lem])\n",
    "                tokens = nltk.word_tokenize(file_lem)\n",
    "                filtered_words = [word for word in tokens if word not in stop_words_list]\n",
    "                for item in filtered_words:\n",
    "                    f.write(\"%s \" % item)\n",
    "                f.write(\"\\n \")\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort and get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ed4270bad291>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"amazon\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# data = pd.read_csv('mr_output/nyt/'+file_name, sep='\\t',header=None, names=['word','count'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mr_co_occ/cc/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "file_name = \"amazon\"\n",
    "# data = pd.read_csv('mr_output/nyt/'+file_name, sep='\\t',header=None, names=['word','count'])\n",
    "data = pd.read_csv('mr_co_occ/cc/'+file_name, sep='\\t',header=None, names=['word','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-91eb708ef1cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.sort_values(by=['count'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## co-occurance json generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_keywords = [\"uber\",\"live\",\"iphone\",\"amazon\",\"time\",\"driver\",\"check\",\"watch\",\"play\",\"love\"]\n",
    "\n",
    "keyword_index = {}\n",
    "\n",
    "count = 0\n",
    "for k in top_keywords:\n",
    "    keyword_index[k] = count\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uber': 0,\n",
       " 'live': 1,\n",
       " 'iphone': 2,\n",
       " 'amazon': 3,\n",
       " 'time': 4,\n",
       " 'driver': 5,\n",
       " 'check': 6,\n",
       " 'watch': 7,\n",
       " 'play': 8,\n",
       " 'love': 9}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"twitter\"\n",
    "\n",
    "data = pd.read_csv('mr_co_occ/'+file_name, sep='\\t',header=None, names=['count','word'])\n",
    "\n",
    "json_dic ={}\n",
    "\n",
    "\n",
    "lst = []\n",
    "for index, row in data.iterrows():\n",
    "    node = {}\n",
    "#     print(row['word'])\n",
    "    in_1 = keyword_index[row['word'].split('-')[0]]\n",
    "    in_2 = keyword_index[row['word'].split('-')[1]]\n",
    "    \n",
    "    node['source'] = in_1\n",
    "    node['target'] = in_2\n",
    "    node['value'] = row['count']\n",
    "    lst.append(node)\n",
    "    \n",
    "json_dic['links'] = lst  \n",
    "\n",
    "lst = []\n",
    "for ky,vl in keyword_index.items():\n",
    "    node = {}\n",
    "    node['group'] = 'humanas'\n",
    "    node['index'] = vl\n",
    "    node['name'] = ky\n",
    "    lst.append(node)\n",
    "    \n",
    "json_dic['nodes'] = lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"links\": [{\"source\": 0, \"target\": 5, \"value\": 5347}, {\"source\": 5, \"target\": 0, \"value\": 5347}, {\"source\": 8, \"target\": 1, \"value\": 2936}, {\"source\": 1, \"target\": 8, \"value\": 2936}, {\"source\": 1, \"target\": 6, \"value\": 2109}, {\"source\": 6, \"target\": 1, \"value\": 2109}, {\"source\": 8, \"target\": 6, \"value\": 2006}, {\"source\": 6, \"target\": 8, \"value\": 2006}, {\"source\": 0, \"target\": 4, \"value\": 1480}, {\"source\": 4, \"target\": 0, \"value\": 1480}, {\"source\": 0, \"target\": 1, \"value\": 730}, {\"source\": 1, \"target\": 0, \"value\": 730}, {\"source\": 7, \"target\": 1, \"value\": 689}, {\"source\": 1, \"target\": 7, \"value\": 689}, {\"source\": 0, \"target\": 8, \"value\": 599}, {\"source\": 8, \"target\": 0, \"value\": 599}, {\"source\": 4, \"target\": 1, \"value\": 563}, {\"source\": 1, \"target\": 4, \"value\": 563}, {\"source\": 7, \"target\": 8, \"value\": 538}, {\"source\": 8, \"target\": 7, \"value\": 538}, {\"source\": 4, \"target\": 2, \"value\": 531}, {\"source\": 2, \"target\": 4, \"value\": 531}, {\"source\": 4, \"target\": 8, \"value\": 506}, {\"source\": 8, \"target\": 4, \"value\": 506}, {\"source\": 0, \"target\": 9, \"value\": 501}, {\"source\": 9, \"target\": 0, \"value\": 501}, {\"source\": 8, \"target\": 9, \"value\": 419}, {\"source\": 9, \"target\": 8, \"value\": 419}, {\"source\": 4, \"target\": 5, \"value\": 411}, {\"source\": 5, \"target\": 4, \"value\": 411}, {\"source\": 4, \"target\": 3, \"value\": 407}, {\"source\": 3, \"target\": 4, \"value\": 407}, {\"source\": 7, \"target\": 4, \"value\": 378}, {\"source\": 4, \"target\": 7, \"value\": 378}, {\"source\": 4, \"target\": 9, \"value\": 374}, {\"source\": 9, \"target\": 4, \"value\": 374}, {\"source\": 9, \"target\": 1, \"value\": 365}, {\"source\": 1, \"target\": 9, \"value\": 365}, {\"source\": 9, \"target\": 3, \"value\": 358}, {\"source\": 3, \"target\": 9, \"value\": 358}, {\"source\": 0, \"target\": 6, \"value\": 355}, {\"source\": 6, \"target\": 0, \"value\": 355}, {\"source\": 6, \"target\": 3, \"value\": 337}, {\"source\": 3, \"target\": 6, \"value\": 337}, {\"source\": 7, \"target\": 0, \"value\": 302}, {\"source\": 7, \"target\": 2, \"value\": 302}, {\"source\": 0, \"target\": 7, \"value\": 302}, {\"source\": 2, \"target\": 7, \"value\": 302}, {\"source\": 4, \"target\": 6, \"value\": 293}, {\"source\": 6, \"target\": 4, \"value\": 293}, {\"source\": 8, \"target\": 2, \"value\": 286}, {\"source\": 2, \"target\": 8, \"value\": 286}, {\"source\": 1, \"target\": 3, \"value\": 281}, {\"source\": 3, \"target\": 1, \"value\": 281}, {\"source\": 8, \"target\": 3, \"value\": 258}, {\"source\": 3, \"target\": 8, \"value\": 258}, {\"source\": 7, \"target\": 9, \"value\": 248}, {\"source\": 9, \"target\": 7, \"value\": 248}, {\"source\": 9, \"target\": 6, \"value\": 246}, {\"source\": 6, \"target\": 9, \"value\": 246}, {\"source\": 9, \"target\": 2, \"value\": 238}, {\"source\": 2, \"target\": 9, \"value\": 238}, {\"source\": 8, \"target\": 5, \"value\": 230}, {\"source\": 5, \"target\": 8, \"value\": 230}, {\"source\": 2, \"target\": 6, \"value\": 207}, {\"source\": 6, \"target\": 2, \"value\": 207}, {\"source\": 0, \"target\": 3, \"value\": 202}, {\"source\": 3, \"target\": 0, \"value\": 202}, {\"source\": 7, \"target\": 6, \"value\": 180}, {\"source\": 6, \"target\": 7, \"value\": 180}, {\"source\": 7, \"target\": 3, \"value\": 179}, {\"source\": 3, \"target\": 7, \"value\": 179}, {\"source\": 1, \"target\": 5, \"value\": 147}, {\"source\": 5, \"target\": 1, \"value\": 147}, {\"source\": 9, \"target\": 5, \"value\": 142}, {\"source\": 5, \"target\": 9, \"value\": 142}, {\"source\": 1, \"target\": 2, \"value\": 127}, {\"source\": 2, \"target\": 1, \"value\": 127}, {\"source\": 5, \"target\": 6, \"value\": 93}, {\"source\": 6, \"target\": 5, \"value\": 93}, {\"source\": 7, \"target\": 5, \"value\": 71}, {\"source\": 2, \"target\": 3, \"value\": 71}, {\"source\": 5, \"target\": 7, \"value\": 71}, {\"source\": 3, \"target\": 2, \"value\": 71}, {\"source\": 0, \"target\": 2, \"value\": 66}, {\"source\": 2, \"target\": 0, \"value\": 66}, {\"source\": 2, \"target\": 5, \"value\": 22}, {\"source\": 5, \"target\": 2, \"value\": 22}, {\"source\": 5, \"target\": 3, \"value\": 22}, {\"source\": 3, \"target\": 5, \"value\": 22}], \"nodes\": [{\"group\": \"humanas\", \"index\": 0, \"name\": \"uber\"}, {\"group\": \"humanas\", \"index\": 1, \"name\": \"live\"}, {\"group\": \"humanas\", \"index\": 2, \"name\": \"iphone\"}, {\"group\": \"humanas\", \"index\": 3, \"name\": \"amazon\"}, {\"group\": \"humanas\", \"index\": 4, \"name\": \"time\"}, {\"group\": \"humanas\", \"index\": 5, \"name\": \"driver\"}, {\"group\": \"humanas\", \"index\": 6, \"name\": \"check\"}, {\"group\": \"humanas\", \"index\": 7, \"name\": \"watch\"}, {\"group\": \"humanas\", \"index\": 8, \"name\": \"play\"}, {\"group\": \"humanas\", \"index\": 9, \"name\": \"love\"}]}'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json_data = json.dumps(json_dic)\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
